{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "21v75FhSkfCq"
   },
   "source": [
    "# Score-Based Generative Modeling\n",
    "\n",
    "\n",
    "### Goals\n",
    "This is a hitchhiker's guide to score-based generative models, a family of approaches based on [estimating gradients of the data distribution](https://arxiv.org/abs/1907.05600). They have obtained high-quality samples comparable to GANs (like below, figure from [this paper](https://arxiv.org/abs/2006.09011)) without requiring adversarial training, and are considered by some to be [the new contender to GANs](https://ajolicoeur.wordpress.com/the-new-contender-to-gans-score-matching-with-langevin-sampling/).\n",
    "\n",
    "![ncsnv2](https://github.com/ermongroup/ncsnv2/blob/master/assets/samples.jpg?raw=true)\n",
    "\n",
    "The contents of this notebook are mainly based on the following paper:\n",
    "\n",
    "Yang Song, Jascha Sohl-Dickstein, Diederik P. Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. \"[Score-Based Generative Modeling through Stochastic Differential Equations.](https://arxiv.org/pdf/2011.13456.pdf)\" Internation Conference on Learning Representations, 2021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XCR6m0HjWGVV"
   },
   "source": [
    "## Introduction\n",
    "\n",
    "### Score and Score-Based Models\n",
    "Given a probablity density function $p(\\mathbf{x})$, we define the *score* as $$\\nabla_\\mathbf{x} \\log p(\\mathbf{x}).$$ As you might guess, score-based generative models are trained to estimate $\\nabla_\\mathbf{x} \\log p(\\mathbf{x})$. Unlike likelihood-based models such as flow models or autoregressive models, score-based models do not have to be normalized and are easier to parameterize. For example, consider a non-normalized statistical model $p_\\theta(\\mathbf{x}) = \\frac{e^{-E_\\theta(\\mathbf{x})}}{Z_\\theta}$, where $E_\\theta(\\mathbf{x}) \\in \\mathbb{R}$ is called the energy function and $Z_\\theta$ is an unknown normalizing constant that makes $p_\\theta(\\mathbf{x})$ a proper probability density function. The energy function is typically parameterized by a flexible neural network. When training it as a likelihood model, we need to know the normalizing constant $Z_\\theta$ by computing complex high-dimensional integrals, which is typically intractable. In constrast, when computing its score, we obtain $\\nabla_\\mathbf{x} \\log p_\\theta(\\mathbf{x}) = -\\nabla_\\mathbf{x} E_\\theta(\\mathbf{x})$ which does not require computing the normalizing constant $Z_\\theta$.\n",
    "\n",
    "In fact, any neural network that maps an input vector $\\mathbf{x} \\in \\mathbb{R}^d$ to an output vector $\\mathbf{y} \\in \\mathbb{R}^d$ can be used as a score-based model, as long as the output and input have the same dimensionality. This yields huge flexibility in choosing model architectures.\n",
    "\n",
    "### Perturbing Data with a Diffusion Process\n",
    "\n",
    "In order to generate samples with score-based models, we need to consider a [diffusion process](https://en.wikipedia.org/wiki/Diffusion_process) that corrupts data slowly into random noise. Scores will arise when we reverse this diffusion process for sample generation. You will see this later in the notebook.\n",
    "\n",
    "A diffusion process is a [stochastic process](https://en.wikipedia.org/wiki/Stochastic_process#:~:text=A%20stochastic%20or%20random%20process%20can%20be%20defined%20as%20a,an%20element%20in%20the%20set.) similar to [Brownian motion](https://en.wikipedia.org/wiki/Brownian_motion). Their paths are like the trajectory of a particle submerged in a flowing fluid, which moves randomly due to unpredictable collisions with other particles. Let $\\{\\mathbf{x}(t) \\in \\mathbb{R}^d \\}_{t=0}^T$ be a diffusion process, indexed by the continuous time variable $t\\in [0,T]$. A diffusion process is governed by a stochastic differential equation (SDE), in the following form\n",
    "\n",
    "\\begin{align*}\n",
    "d \\mathbf{x} = \\mathbf{f}(\\mathbf{x}, t) d t + g(t) d \\mathbf{w},\n",
    "\\end{align*}\n",
    "\n",
    "where $\\mathbf{f}(\\cdot, t): \\mathbb{R}^d \\to \\mathbb{R}^d$ is called the *drift coefficient* of the SDE, $g(t) \\in \\mathbb{R}$ is called the *diffusion coefficient*, and $\\mathbf{w}$ represents the standard Brownian motion. You can understand an SDE as a stochastic generalization to ordinary differential equations (ODEs). Particles moving according to an SDE not only follows the deterministic drift $\\mathbf{f}(\\mathbf{x}, t)$, but are also affected by the random noise coming from $g(t) d\\mathbf{w}$. From now on, we use $p_t(\\mathbf{x})$ to denote the distribution of $\\mathbf{x}(t)$.\n",
    "\n",
    "For score-based generative modeling, we will choose a diffusion process such that $\\mathbf{x}(0) \\sim p_0$, and $\\mathbf{x}(T) \\sim p_T$. Here $p_0$ is the data distribution where we have a dataset of i.i.d. samples, and $p_T$ is the prior distribution that has a tractable form and easy to sample from. The noise perturbation by the diffusion process is large enough to ensure $p_T$ does not depend on $p_0$.\n",
    "\n",
    "### Reversing the Diffusion Process Yields Score-Based Generative Models\n",
    "By starting from a sample from the prior distribution $p_T$ and reversing the diffusion process, we will be able to obtain a sample from the data distribution $p_0$. Crucially, the reverse process is a diffusion process running backwards in time. It is given by the following reverse-time SDE\n",
    "\n",
    "\\begin{align}\n",
    "  d\\mathbf{x} = [\\mathbf{f}(\\mathbf{x}, t) - g^2(t)\\nabla_{\\mathbf{x}}\\log p_t(\\mathbf{x})] dt + g(t) d\\bar{\\mathbf{w}},\n",
    "\\end{align}\n",
    "\n",
    "where $\\bar{\\mathbf{w}}$ is a Brownian motion in the reverse time direction, and $dt$ represents an infinitesimal negative time step. This reverse SDE can be computed once we know the drift and diffusion coefficients of the forward SDE, as well as the score of $p_t(\\mathbf{x})$ for each $t\\in[0, T]$.\n",
    "\n",
    "The overall intuition of score-based generative modeling with SDEs can be summarized in the illustration below\n",
    "\n",
    "![sde schematic](https://drive.google.com/uc?id=1Ptvb790eQRYMHLnDGBeYZK9A2cF-JMEP)\n",
    "\n",
    "\n",
    "### Score Estimation\n",
    "\n",
    "Based on the above intuition, we can use the time-dependent score function $\\nabla_\\mathbf{x} \\log p_t(\\mathbf{x})$ to construct the reverse-time SDE, and then solve it numerically to obtain samples from $p_0$ using samples from a prior distribution $p_T$. We can train a time-dependent score-based model $s_\\theta(\\mathbf{x}, t)$ to approximate $\\nabla_\\mathbf{x} \\log p_t(\\mathbf{x})$, using the following weighted sum of [denoising score matching](http://www.iro.umontreal.ca/~vincentp/Publications/smdae_techreport.pdf) objectives.\n",
    "\n",
    "\\begin{align}\n",
    "\\min_\\theta \\mathbb{E}_{t\\sim \\mathcal{U}(0, T)} [\\lambda(t) \\mathbb{E}_{\\mathbf{x}(0) \\sim p_0(\\mathbf{x})}\\mathbf{E}_{\\mathbf{x}(t) \\sim p_{0t}(\\mathbf{x}(t) \\mid \\mathbf{x}(0))}[ \\|s_\\theta(\\mathbf{x}(t), t) - \\nabla_{\\mathbf{x}(t)}\\log p_{0t}(\\mathbf{x}(t) \\mid \\mathbf{x}(0))\\|_2^2]],\n",
    "\\end{align}\n",
    "where $\\mathcal{U}(0,T)$ is a uniform distribution over $[0, T]$, $p_{0t}(\\mathbf{x}(t) \\mid \\mathbf{x}(0))$ denotes the transition probability from $\\mathbf{x}(0)$ to $\\mathbf{x}(t)$, and $\\lambda(t) \\in \\mathbb{R}_{>0}$ denotes a positive weighting function.\n",
    "\n",
    "In the objective, the expectation over $\\mathbf{x}(0)$ can be estimated with empirical means over data samples from $p_0$. The expectation over $\\mathbf{x}(t)$ can be estimated by sampling from $p_{0t}(\\mathbf{x}(t) \\mid \\mathbf{x}(0))$, which is efficient when the drift coefficient $\\mathbf{f}(\\mathbf{x}, t)$ is affine. The weight function $\\lambda(t)$ is typically chosen to be inverse proportional to $\\mathbb{E}[\\|\\nabla_{\\mathbf{x}}\\log p_{0t}(\\mathbf{x}(t) \\mid \\mathbf{x}(0)) \\|_2^2]$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GFuMaPov5HlV"
   },
   "source": [
    "### Time-Dependent Score-Based Model\n",
    "\n",
    "There are no restrictions on the network architecture of time-dependent score-based models, except that their output should have the same dimensionality as the input, and they should be conditioned on time.\n",
    "\n",
    "Several useful tips on architecture choice:\n",
    "* It usually performs well to use the [U-net](https://arxiv.org/abs/1505.04597) architecture as the backbone of the score network $s_\\theta(\\mathbf{x}, t)$,\n",
    "\n",
    "* We can incorporate the time information via [Gaussian random features](https://arxiv.org/abs/2006.10739). Specifically, we first sample $\\omega \\sim \\mathcal{N}(\\mathbf{0}, s^2\\mathbf{I})$ which is subsequently fixed for the model (i.e., not learnable). For a time step $t$, the corresponding Gaussian random feature is defined as\n",
    "\\begin{align}\n",
    "  [\\sin(2\\pi \\omega t) ; \\cos(2\\pi \\omega t)],\n",
    "\\end{align}\n",
    "where $[\\vec{a} ; \\vec{b}]$ denotes the concatenation of vector $\\vec{a}$ and $\\vec{b}$. This Gaussian random feature can be used as an encoding for time step $t$ so that the score network can condition on $t$ by incorporating this encoding. We will see this further in the code.\n",
    "\n",
    "* We can rescale the output of the U-net by $1/\\sqrt{\\mathbb{E}[\\|\\nabla_{\\mathbf{x}}\\log p_{0t}(\\mathbf{x}(t) \\mid \\mathbf{x}(0)) \\|_2^2]}$. This is because the optimal $s_\\theta(\\mathbf{x}(t), t)$ has an $\\ell_2$-norm close to $\\mathbb{E}[\\|\\nabla_{\\mathbf{x}}\\log p_{0t}(\\mathbf{x}(t) \\mid \\mathbf{x}(0))]\\|_2$, and the rescaling helps capture the norm of the true score. Recall that the training objective contains sums of the form\n",
    "\\begin{align*}\n",
    "\\mathbf{E}_{\\mathbf{x}(t) \\sim p_{0t}(\\mathbf{x}(t) \\mid \\mathbf{x}(0))}[ \\|s_\\theta(\\mathbf{x}(t), t) - \\nabla_{\\mathbf{x}(t)}\\log p_{0t}(\\mathbf{x}(t) \\mid \\mathbf{x}(0))\\|_2^2].\n",
    "\\end{align*}\n",
    "Therefore, it is natural to expect that the optimal score model $s_\\theta(\\mathbf{x}, t) \\approx \\nabla_{\\mathbf{x}(t)} \\log p_{0t}(\\mathbf{x}(t) \\mid \\mathbf{x}(0))$.\n",
    "\n",
    "* Use [exponential moving average](https://discuss.pytorch.org/t/how-to-apply-exponential-moving-average-decay-for-variables/10856/3) (EMA) of weights when sampling. This can greatly improve sample quality, but requires slightly longer training time, and requires more work in implementation. We do not include this in this tutorial, but highly recommend it when you employ score-based generative modeling to tackle more challenging real problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "cellView": "form",
    "id": "YyQtV7155Nht"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "class GaussianFourierProjection(nn.Module):\n",
    "    \"\"\"Gaussian random features for encoding time steps.\"\"\"\n",
    "    def __init__(self, embed_dim, scale=30.):\n",
    "        super().__init__()\n",
    "        self.W = nn.Parameter(torch.randn(embed_dim // 2) * scale, requires_grad=False)\n",
    "    def forward(self, x):\n",
    "        x_proj = x[:, None] * self.W[None, :] * 2 * np.pi\n",
    "        return torch.cat([torch.sin(x_proj), torch.cos(x_proj)], dim=-1)\n",
    "\n",
    "class Dense(nn.Module):\n",
    "    \"\"\"A fully connected layer that reshapes outputs to feature maps.\"\"\"\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(input_dim, output_dim)\n",
    "    def forward(self, x):\n",
    "        return self.dense(x)[..., None, None]\n",
    "\n",
    "class ScoreNet(nn.Module):\n",
    "    \"\"\"A time-dependent score-based model for smaller inputs (e.g., 10x10).\"\"\"\n",
    "\n",
    "    def __init__(self, marginal_prob_std, channels=[32, 64, 128], embed_dim=256):\n",
    "        super().__init__()\n",
    "        self.marginal_prob_std = marginal_prob_std\n",
    "        # Time embedding\n",
    "        self.embed = nn.Sequential(\n",
    "            GaussianFourierProjection(embed_dim=embed_dim),\n",
    "            nn.Linear(embed_dim, embed_dim)\n",
    "        )\n",
    "\n",
    "        # Encoder\n",
    "        self.conv1 = nn.Conv2d(1, channels[0], 3, stride=1, padding=1, bias=False)\n",
    "        self.dense1 = Dense(embed_dim, channels[0])\n",
    "        self.gnorm1 = nn.GroupNorm(4, num_channels=channels[0])\n",
    "\n",
    "        self.conv2 = nn.Conv2d(channels[0], channels[1], 3, stride=2, padding=1, bias=False)  \n",
    "        self.dense2 = Dense(embed_dim, channels[1])\n",
    "        self.gnorm2 = nn.GroupNorm(32, num_channels=channels[1])\n",
    "\n",
    "        self.conv3 = nn.Conv2d(channels[1], channels[2], 3, stride=2, padding=1, bias=False)\n",
    "        self.dense3 = Dense(embed_dim, channels[2])\n",
    "        self.gnorm3 = nn.GroupNorm(32, num_channels=channels[2])\n",
    "\n",
    "        # Decoder\n",
    "        # Going from 3x3 back to 5x5\n",
    "        self.tconv3 = nn.ConvTranspose2d(channels[2], channels[1], 3, stride=2, padding=1, output_padding=0, bias=False)\n",
    "        self.dense4 = Dense(embed_dim, channels[1])\n",
    "        self.tgnorm3 = nn.GroupNorm(32, num_channels=channels[1])\n",
    "\n",
    "        # Going from 5x5 back to 10x10\n",
    "        self.tconv2 = nn.ConvTranspose2d(channels[1] + channels[1], channels[0], 3, stride=2, padding=1, output_padding=1, bias=False)\n",
    "        self.dense5 = Dense(embed_dim, channels[0])\n",
    "        self.tgnorm2 = nn.GroupNorm(32, num_channels=channels[0])\n",
    "\n",
    "        # Final layer to get back to 10x10 from 10x10\n",
    "        self.tconv1 = nn.ConvTranspose2d(channels[0] + channels[0], 1, 3, stride=1, padding=1)\n",
    "        self.dense6 = Dense(embed_dim, 1)\n",
    "\n",
    "        self.act = lambda x: x * torch.sigmoid(x)\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        # Embedding for time\n",
    "        embed = self.act(self.embed(t))\n",
    "\n",
    "        # Encoding path\n",
    "        # Stage 1\n",
    "        h1 = self.conv1(x)\n",
    "        h1 += self.dense1(embed)\n",
    "        h1 = self.gnorm1(h1)\n",
    "        h1 = self.act(h1)\n",
    "\n",
    "        # Stage 2\n",
    "        h2 = self.conv2(h1)\n",
    "        h2 += self.dense2(embed)\n",
    "        h2 = self.gnorm2(h2)\n",
    "        h2 = self.act(h2)\n",
    "\n",
    "        # Stage 3\n",
    "        h3 = self.conv3(h2)\n",
    "        h3 += self.dense3(embed)\n",
    "        h3 = self.gnorm3(h3)\n",
    "        h3 = self.act(h3)\n",
    "\n",
    "        # Decoding path\n",
    "        # From h3 (3x3) to h2 (5x5)\n",
    "        h = self.tconv3(h3)\n",
    "        h += self.dense4(embed)\n",
    "        h = self.tgnorm3(h)\n",
    "        h = self.act(h)\n",
    "        h = torch.cat([h, h2], dim=1)\n",
    "\n",
    "        # From 5x5 back to 10x10\n",
    "        h = self.tconv2(h)\n",
    "        h += self.dense5(embed)\n",
    "        h = self.tgnorm2(h)\n",
    "        h = self.act(h)\n",
    "        h = torch.cat([h, h1], dim=1)\n",
    "\n",
    "        # Final layer (10x10 -> 10x10)\n",
    "        h = self.tconv1(h)\n",
    "        # Add a final embed as well (optional)\n",
    "        h += self.dense6(embed)\n",
    "        # Normalize output\n",
    "        h = h / self.marginal_prob_std(t)[:, None, None, None]\n",
    "\n",
    "        return h\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PpJSwfyY6mJz"
   },
   "source": [
    "## Training with Weighted Sum of Denoising Score Matching Objectives\n",
    "\n",
    "Now let's get our hands dirty on training. First of all, we need to specify an SDE that perturbs the data distribution $p_0$ to a prior distribution $p_T$. We choose the following SDE\n",
    "\\begin{align*}\n",
    "d \\mathbf{x} = \\sigma^t d\\mathbf{w}, \\quad t\\in[0,1]\n",
    "\\end{align*}\n",
    "In this case,\n",
    "\\begin{align*}\n",
    "p_{0t}(\\mathbf{x}(t) \\mid \\mathbf{x}(0)) = \\mathcal{N}\\bigg(\\mathbf{x}(t); \\mathbf{x}(0), \\frac{1}{2\\log \\sigma}(\\sigma^{2t} - 1) \\mathbf{I}\\bigg)\n",
    "\\end{align*}\n",
    "and we can choose the weighting function $\\lambda(t) = \\frac{1}{2 \\log \\sigma}(\\sigma^{2t} - 1)$.\n",
    "\n",
    "When $\\sigma$ is large, the prior distribution, $p_{t=1}$ is\n",
    "\\begin{align*}\n",
    "\\int p_0(\\mathbf{y})\\mathcal{N}\\bigg(\\mathbf{x}; \\mathbf{y}, \\frac{1}{2 \\log \\sigma}(\\sigma^2 - 1)\\mathbf{I}\\bigg) d \\mathbf{y} \\approx \\mathbf{N}\\bigg(\\mathbf{x}; \\mathbf{0}, \\frac{1}{2 \\log \\sigma}(\\sigma^2 - 1)\\mathbf{I}\\bigg),\n",
    "\\end{align*}\n",
    "which is approximately independent of the data distribution and is easy to sample from.\n",
    "\n",
    "Intuitively, this SDE captures a continuum of Gaussian perturbations with variance function $\\frac{1}{2 \\log \\sigma}(\\sigma^{2t} - 1)$. This continuum of perturbations allows us to gradually transfer samples from a data distribution $p_0$ to a simple Gaussian distribution $p_1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "cellView": "form",
    "id": "LZC7wrOvxLdL"
   },
   "outputs": [],
   "source": [
    "#@title Set up the SDE\n",
    "\n",
    "import functools\n",
    "device = 'cuda' #@param ['cuda', 'cpu'] {'type':'string'}\n",
    "\n",
    "def marginal_prob_std(t, sigma):\n",
    "  \"\"\"Compute the mean and standard deviation of $p_{0t}(x(t) | x(0))$.\n",
    "\n",
    "  Args:\n",
    "    t: A vector of time steps.\n",
    "    sigma: The $\\sigma$ in our SDE.\n",
    "\n",
    "  Returns:\n",
    "    The standard deviation.\n",
    "  \"\"\"\n",
    "  t = torch.tensor(t, device=device)\n",
    "  return torch.sqrt((sigma**(2 * t) - 1.) / 2. / np.log(sigma))\n",
    "\n",
    "def diffusion_coeff(t, sigma):\n",
    "  \"\"\"Compute the diffusion coefficient of our SDE.\n",
    "\n",
    "  Args:\n",
    "    t: A vector of time steps.\n",
    "    sigma: The $\\sigma$ in our SDE.\n",
    "\n",
    "  Returns:\n",
    "    The vector of diffusion coefficients.\n",
    "  \"\"\"\n",
    "  return torch.tensor(sigma**t, device=device)\n",
    "\n",
    "sigma =  25.0#@param {'type':'number'}\n",
    "marginal_prob_std_fn = functools.partial(marginal_prob_std, sigma=sigma)\n",
    "diffusion_coeff_fn = functools.partial(diffusion_coeff, sigma=sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "cellView": "form",
    "id": "zOsoqPdXHuL5"
   },
   "outputs": [],
   "source": [
    "#@title Define the loss function (double click to expand or collapse)\n",
    "\n",
    "def loss_fn(model, x, marginal_prob_std, eps=1e-5):\n",
    "  \"\"\"The loss function for training score-based generative models.\n",
    "\n",
    "  Args:\n",
    "    model: A PyTorch model instance that represents a\n",
    "      time-dependent score-based model.\n",
    "    x: A mini-batch of training data.\n",
    "    marginal_prob_std: A function that gives the standard deviation of\n",
    "      the perturbation kernel.\n",
    "    eps: A tolerance value for numerical stability.\n",
    "  \"\"\"\n",
    "  random_t = torch.rand(x.shape[0], device=x.device) * (1. - eps) + eps\n",
    "  z = torch.randn_like(x)\n",
    "  std = marginal_prob_std(random_t)\n",
    "  perturbed_x = x + z * std[:, None, None, None]\n",
    "  score = model(perturbed_x, random_t)\n",
    "  loss = torch.mean(torch.sum((score * std[:, None, None, None] + z)**2, dim=(1,2,3)))\n",
    "  return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 120,
     "referenced_widgets": [
      "b1308d495dd643b48378ff372c8f9c3a",
      "c14794e9abf641a9884a0dd618e14d1e",
      "6f6b56e6662149caad3e30815c25e374",
      "6702c584e95347ae8d425257b8ec540e",
      "cdf00304d2da46a1bfc2e86542882db4",
      "d909e838a6894af4abe248ea52bbeb93",
      "61a88f148b8e48adb16bccd7db3eeaf8",
      "6fd034cfc2b74649ae887af76e13cbe5",
      "c4de8fe7e05142a78560e13c2b1ca385",
      "81a3a9c9a6b742fa9f8f19f71fedd73a",
      "64b0fddd0f2544468c028b6da780515c"
     ]
    },
    "id": "8PPsLx4dGCGa",
    "outputId": "3033feda-6b41-4a98-8408-14ee8e47bb01"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11770778a4f94560bab000c134fcf3aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_48351/2246265617.py:16: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(t, device=device)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [32, 1, 3, 3], expected input[4096, 4, 10, 10] to have 1 channels, but got 4 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[63], line 30\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m data_loader:\n\u001b[1;32m     29\u001b[0m   x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mfloat32)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 30\u001b[0m   loss \u001b[38;5;241m=\u001b[39m \u001b[43mloss_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscore_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmarginal_prob_std_fn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m   optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     32\u001b[0m   loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "Cell \u001b[0;32mIn[13], line 18\u001b[0m, in \u001b[0;36mloss_fn\u001b[0;34m(model, x, marginal_prob_std, eps)\u001b[0m\n\u001b[1;32m     16\u001b[0m std \u001b[38;5;241m=\u001b[39m marginal_prob_std(random_t)\n\u001b[1;32m     17\u001b[0m perturbed_x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m z \u001b[38;5;241m*\u001b[39m std[:, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[0;32m---> 18\u001b[0m score \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mperturbed_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_t\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m loss \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmean(torch\u001b[38;5;241m.\u001b[39msum((score \u001b[38;5;241m*\u001b[39m std[:, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m] \u001b[38;5;241m+\u001b[39m z)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m, dim\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m3\u001b[39m)))\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/miniforge3/envs/torch/lib/python3.13/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/torch/lib/python3.13/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniforge3/envs/torch/lib/python3.13/site-packages/torch/nn/parallel/data_parallel.py:191\u001b[0m, in \u001b[0;36mDataParallel.forward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    188\u001b[0m     module_kwargs \u001b[38;5;241m=\u001b[39m ({},)\n\u001b[1;32m    190\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice_ids) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 191\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodule_kwargs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    192\u001b[0m replicas \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplicate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice_ids[: \u001b[38;5;28mlen\u001b[39m(inputs)])\n\u001b[1;32m    193\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparallel_apply(replicas, inputs, module_kwargs)\n",
      "File \u001b[0;32m~/miniforge3/envs/torch/lib/python3.13/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/torch/lib/python3.13/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[11], line 71\u001b[0m, in \u001b[0;36mScoreNet.forward\u001b[0;34m(self, x, t)\u001b[0m\n\u001b[1;32m     67\u001b[0m embed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed(t))\n\u001b[1;32m     69\u001b[0m \u001b[38;5;66;03m# Encoding path\u001b[39;00m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;66;03m# Stage 1\u001b[39;00m\n\u001b[0;32m---> 71\u001b[0m h1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     72\u001b[0m h1 \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdense1(embed)\n\u001b[1;32m     73\u001b[0m h1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgnorm1(h1)\n",
      "File \u001b[0;32m~/miniforge3/envs/torch/lib/python3.13/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/torch/lib/python3.13/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniforge3/envs/torch/lib/python3.13/site-packages/torch/nn/modules/conv.py:554\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 554\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/torch/lib/python3.13/site-packages/torch/nn/modules/conv.py:549\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    538\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(\n\u001b[1;32m    539\u001b[0m         F\u001b[38;5;241m.\u001b[39mpad(\n\u001b[1;32m    540\u001b[0m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    547\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups,\n\u001b[1;32m    548\u001b[0m     )\n\u001b[0;32m--> 549\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    550\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\n\u001b[1;32m    551\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [32, 1, 3, 3], expected input[4096, 4, 10, 10] to have 1 channels, but got 4 channels instead"
     ]
    }
   ],
   "source": [
    "#@title Training (double click to expand or collapse)\n",
    "\n",
    "import torch\n",
    "import functools\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import notebook\n",
    "from utils import ValueMapData\n",
    "\n",
    "\n",
    "score_model = torch.nn.DataParallel(ScoreNet(marginal_prob_std=marginal_prob_std_fn))\n",
    "score_model = score_model.to(device)\n",
    "\n",
    "n_epochs =   10#@param {'type':'integer'}\n",
    "## size of a mini-batch\n",
    "batch_size =  4096#@param {'type':'integer'}\n",
    "## learning rate\n",
    "lr=1e-4 #@param {'type':'number'}\n",
    "\n",
    "dataset = ValueMapData('./data/p1')\n",
    "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=12)\n",
    "\n",
    "optimizer = Adam(score_model.parameters(), lr=lr)\n",
    "tqdm_epoch = notebook.trange(n_epochs)\n",
    "for epoch in tqdm_epoch:\n",
    "  avg_loss = 0.\n",
    "  num_items = 0\n",
    "  for x in data_loader:\n",
    "    x = x.to(torch.float32).to(device).mean(1, keepdim=True)\n",
    "    loss = loss_fn(score_model, x, marginal_prob_std_fn)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    avg_loss += loss.item() * x.shape[0]\n",
    "    num_items += x.shape[0]\n",
    "  # Print the averaged training loss so far.\n",
    "  tqdm_epoch.set_description('Average Loss: {:5f}'.format(avg_loss / num_items))\n",
    "  # Update the checkpoint after each epoch of training.\n",
    "  torch.save(score_model.state_dict(), 'vmap_model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tldaUHUtHuej"
   },
   "source": [
    "## Sampling with Numerical SDE Solvers\n",
    "Recall that for any SDE of the form\n",
    "\\begin{align*}\n",
    "d \\mathbf{x} = \\mathbf{f}(\\mathbf{x}, t) dt + g(t) d\\mathbf{w},\n",
    "\\end{align*}\n",
    "the reverse-time SDE is given by\n",
    "\\begin{align*}\n",
    "d \\mathbf{x} = [\\mathbf{f}(\\mathbf{x}, t) - g(t)^2 \\nabla_\\mathbf{x} \\log p_t(\\mathbf{x})] dt + g(t) d \\bar{\\mathbf{w}}.\n",
    "\\end{align*}\n",
    "Since we have chosen the forward SDE to be\n",
    "\\begin{align*}\n",
    "d \\mathbf{x} = \\sigma^t d\\mathbf{w}, \\quad t\\in[0,1]\n",
    "\\end{align*}\n",
    "The reverse-time SDE is given by\n",
    "\\begin{align*}\n",
    "d\\mathbf{x} = -\\sigma^{2t} \\nabla_\\mathbf{x} \\log p_t(\\mathbf{x}) dt + \\sigma^t d \\bar{\\mathbf{w}}.\n",
    "\\end{align*}\n",
    "To sample from our time-dependent score-based model $s_\\theta(\\mathbf{x}, t)$, we first draw a sample from the prior distribution $p_1 \\approx \\mathbf{N}\\bigg(\\mathbf{x}; \\mathbf{0}, \\frac{1}{2}(\\sigma^{2} - 1) \\mathbf{I}\\bigg)$, and then solve the reverse-time SDE with numerical methods.\n",
    "\n",
    "In particular, using our time-dependent score-based model, the reverse-time SDE can be approximated by\n",
    "\\begin{align*}\n",
    "d\\mathbf{x} = -\\sigma^{2t} s_\\theta(\\mathbf{x}, t) dt + \\sigma^t d \\bar{\\mathbf{w}}\n",
    "\\end{align*}\n",
    "\n",
    "Next, one can use numerical methods to solve for the reverse-time SDE, such as the [Euler-Maruyama](https://en.wikipedia.org/wiki/Euler%E2%80%93Maruyama_method) approach. It is based on a simple discretization to the SDE, replacing $dt$ with $\\Delta t$ and $d \\mathbf{w}$ with $\\mathbf{z} \\sim \\mathcal{N}(\\mathbf{0}, g^2(t) \\Delta t \\mathbf{I})$. When applied to our reverse-time SDE, we can obtain the following iteration rule\n",
    "\\begin{align}\n",
    "\\mathbf{x}_{t-\\Delta t} = \\mathbf{x}_t + \\sigma^{2t} s_\\theta(\\mathbf{x}_t, t)\\Delta t + \\sigma^t\\sqrt{\\Delta t} \\mathbf{z}_t,\n",
    "\\end{align}\n",
    "where $\\mathbf{z}_t \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I})$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "cellView": "form",
    "id": "6FxBTOSSH2QR"
   },
   "outputs": [],
   "source": [
    "#@title Define the Euler-Maruyama sampler (double click to expand or collapse)\n",
    "\n",
    "## The number of sampling steps.\n",
    "num_steps =  500#@param {'type':'integer'}\n",
    "def Euler_Maruyama_sampler(score_model,\n",
    "                           marginal_prob_std,\n",
    "                           diffusion_coeff,\n",
    "                           batch_size=64,\n",
    "                           num_steps=num_steps,\n",
    "                           device='cuda',\n",
    "                           eps=1e-3):\n",
    "  \"\"\"Generate samples from score-based models with the Euler-Maruyama solver.\n",
    "\n",
    "  Args:\n",
    "    score_model: A PyTorch model that represents the time-dependent score-based model.\n",
    "    marginal_prob_std: A function that gives the standard deviation of\n",
    "      the perturbation kernel.\n",
    "    diffusion_coeff: A function that gives the diffusion coefficient of the SDE.\n",
    "    batch_size: The number of samplers to generate by calling this function once.\n",
    "    num_steps: The number of sampling steps.\n",
    "      Equivalent to the number of discretized time steps.\n",
    "    device: 'cuda' for running on GPUs, and 'cpu' for running on CPUs.\n",
    "    eps: The smallest time step for numerical stability.\n",
    "\n",
    "  Returns:\n",
    "    Samples.\n",
    "  \"\"\"\n",
    "  t = torch.ones(batch_size, device=device)\n",
    "  init_x = torch.randn(batch_size, 1, 10, 10, device=device) \\\n",
    "    * marginal_prob_std(t)[:, None, None, None]\n",
    "  time_steps = torch.linspace(1., eps, num_steps, device=device)\n",
    "  step_size = time_steps[0] - time_steps[1]\n",
    "  x = init_x\n",
    "  with torch.no_grad():\n",
    "    for time_step in notebook.tqdm(time_steps):\n",
    "      batch_time_step = torch.ones(batch_size, device=device) * time_step\n",
    "      g = diffusion_coeff(batch_time_step)\n",
    "      mean_x = x + (g**2)[:, None, None, None] * score_model(x, batch_time_step) * step_size\n",
    "      x = mean_x + torch.sqrt(step_size) * g[:, None, None, None] * torch.randn_like(x)\n",
    "  # Do not include any noise in the last sampling step.\n",
    "  return mean_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DC6QVkUQvFyB"
   },
   "source": [
    "## Sampling with Predictor-Corrector Methods\n",
    "\n",
    "Aside from generic numerical SDE solvers, we can leverage special properties of our reverse-time SDE for better solutions. Since we have an estimate of the score of $p_t(\\mathbf{x}(t))$ via the score-based model, i.e., $s_\\theta(\\mathbf{x}, t) \\approx \\nabla_{\\mathbf{x}(t)} \\log p_t(\\mathbf{x}(t))$, we can leverage score-based MCMC approaches, such as Langevin MCMC, to correct the solution obtained by numerical SDE solvers.\n",
    "\n",
    "Score-based MCMC approaches can produce samples from a distribution $p(\\mathbf{x})$ once its score $\\nabla_\\mathbf{x} \\log p(\\mathbf{x})$ is known. For example, Langevin MCMC operates by running the following iteration rule for $i=1,2,\\cdots, N$:\n",
    "\\begin{align*}\n",
    "\\mathbf{x}_{i+1} = \\mathbf{x}_{i} + \\epsilon \\nabla_\\mathbf{x} \\log p(\\mathbf{x}_i) + \\sqrt{2\\epsilon} \\mathbf{z}_i,\n",
    "\\end{align*}\n",
    "where $\\mathbf{z}_i \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I})$, $\\epsilon > 0$ is the step size, and $\\mathbf{x}_1$ is initialized from any prior distribution $\\pi(\\mathbf{x}_1)$. When $N\\to\\infty$ and $\\epsilon \\to 0$, the final value $\\mathbf{x}_{N+1}$ becomes a sample from $p(\\mathbf{x})$ under some regularity conditions. Therefore, given $s_\\theta(\\mathbf{x}, t) \\approx \\nabla_\\mathbf{x} \\log p_t(\\mathbf{x})$, we can get an approximate sample from $p_t(\\mathbf{x})$ by running several steps of Langevin MCMC, replacing $\\nabla_\\mathbf{x} \\log p_t(\\mathbf{x})$ with $s_\\theta(\\mathbf{x}, t)$ in the iteration rule.\n",
    "\n",
    "Predictor-Corrector samplers combine both numerical solvers for the reverse-time SDE and the Langevin MCMC approach. In particular, we first apply one step of numerical SDE solver to obtain $\\mathbf{x}_{t-\\Delta t}$ from $\\mathbf{x}_t$, which is called the \"predictor\" step. Next, we apply several steps of Langevin MCMC to refine $\\mathbf{x}_t$, such that $\\mathbf{x}_t$ becomes a more accurate sample from $p_{t-\\Delta t}(\\mathbf{x})$. This is the \"corrector\" step as the MCMC helps reduce the error of the numerical SDE solver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "cellView": "form",
    "id": "qW1HaPZb9gDM"
   },
   "outputs": [],
   "source": [
    "#@title Define the Predictor-Corrector sampler (double click to expand or collapse)\n",
    "\n",
    "signal_to_noise_ratio = 0.16 #@param {'type':'number'}\n",
    "\n",
    "## The number of sampling steps.\n",
    "num_steps =  500#@param {'type':'integer'}\n",
    "def pc_sampler(score_model,\n",
    "               marginal_prob_std,\n",
    "               diffusion_coeff,\n",
    "               batch_size=64,\n",
    "               num_steps=num_steps,\n",
    "               snr=signal_to_noise_ratio,\n",
    "               device='cuda',\n",
    "               eps=1e-3):\n",
    "  \"\"\"Generate samples from score-based models with Predictor-Corrector method.\n",
    "\n",
    "  Args:\n",
    "    score_model: A PyTorch model that represents the time-dependent score-based model.\n",
    "    marginal_prob_std: A function that gives the standard deviation\n",
    "      of the perturbation kernel.\n",
    "    diffusion_coeff: A function that gives the diffusion coefficient\n",
    "      of the SDE.\n",
    "    batch_size: The number of samplers to generate by calling this function once.\n",
    "    num_steps: The number of sampling steps.\n",
    "      Equivalent to the number of discretized time steps.\n",
    "    device: 'cuda' for running on GPUs, and 'cpu' for running on CPUs.\n",
    "    eps: The smallest time step for numerical stability.\n",
    "\n",
    "  Returns:\n",
    "    Samples.\n",
    "  \"\"\"\n",
    "  t = torch.ones(batch_size, device=device)\n",
    "  init_x = torch.randn(batch_size, 1, 10, 10, device=device) * marginal_prob_std(t)[:, None, None, None]\n",
    "  time_steps = np.linspace(1., eps, num_steps)\n",
    "  step_size = time_steps[0] - time_steps[1]\n",
    "  x = init_x\n",
    "  with torch.no_grad():\n",
    "    for time_step in notebook.tqdm(time_steps):\n",
    "      batch_time_step = torch.ones(batch_size, device=device) * time_step\n",
    "      # Corrector step (Langevin MCMC)\n",
    "      grad = score_model(x, batch_time_step)\n",
    "      grad_norm = torch.norm(grad.reshape(grad.shape[0], -1), dim=-1).mean()\n",
    "      noise_norm = np.sqrt(np.prod(x.shape[1:]))\n",
    "      langevin_step_size = 2 * (snr * noise_norm / grad_norm)**2\n",
    "      x = x + langevin_step_size * grad + torch.sqrt(2 * langevin_step_size) * torch.randn_like(x)\n",
    "\n",
    "      # Predictor step (Euler-Maruyama)\n",
    "      g = diffusion_coeff(batch_time_step)\n",
    "      x_mean = x + (g**2)[:, None, None, None] * score_model(x, batch_time_step) * step_size\n",
    "      x = x_mean + torch.sqrt(g**2 * step_size)[:, None, None, None] * torch.randn_like(x)\n",
    "\n",
    "    # The last step does not include any noise\n",
    "    return x_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0PdMMadpUbrj"
   },
   "source": [
    "## Sampling with Numerical ODE Solvers\n",
    "\n",
    "For any SDE of the form\n",
    "\\begin{align*}\n",
    "d \\mathbf{x} = \\mathbf{f}(\\mathbf{x}, t) d t + g(t) d \\mathbf{w},\n",
    "\\end{align*}\n",
    "there exists an associated ordinary differential equation (ODE)\n",
    "\\begin{align*}\n",
    "d \\mathbf{x} = \\bigg[\\mathbf{f}(\\mathbf{x}, t) - \\frac{1}{2}g(t)^2 \\nabla_\\mathbf{x} \\log p_t(\\mathbf{x})\\bigg] dt,\n",
    "\\end{align*}\n",
    "such that their trajectories have the same mariginal probability density $p_t(\\mathbf{x})$. Therefore, by solving this ODE in the reverse time direction, we can sample from the same distribution as solving the reverse-time SDE.\n",
    "We call this ODE the *probability flow ODE*.\n",
    "\n",
    "Below is a schematic figure showing how trajectories from this probability flow ODE differ from SDE trajectories, while still sampling from the same distribution.\n",
    "![SDE and ODE](https://drive.google.com/uc?id=1CGFbtY2mCjlIY8pjvoGevfa_32d4b1dj)\n",
    "\n",
    "Therefore, we can start from a sample from $p_T$, integrate the ODE in the reverse time direction, and then get a sample from $p_0$. In particular, for the SDE in our running example, we can integrate the following ODE from $t=T$ to $0$ for sample generation\n",
    "\\begin{align*}\n",
    "d\\mathbf{x} =  -\\frac{1}{2}\\sigma^{2t} s_\\theta(\\mathbf{x}, t) dt.\n",
    "\\end{align*}\n",
    "This can be done using many black-box ODE solvers provided by packages such as `scipy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "cellView": "form",
    "id": "nxrCTFM8CfDN"
   },
   "outputs": [],
   "source": [
    "#@title Define the ODE sampler (double click to expand or collapse)\n",
    "\n",
    "from scipy import integrate\n",
    "\n",
    "## The error tolerance for the black-box ODE solver\n",
    "error_tolerance = 1e-5 #@param {'type': 'number'}\n",
    "def ode_sampler(score_model,\n",
    "                marginal_prob_std,\n",
    "                diffusion_coeff,\n",
    "                batch_size=64,\n",
    "                atol=error_tolerance,\n",
    "                rtol=error_tolerance,\n",
    "                device='cuda',\n",
    "                z=None,\n",
    "                eps=1e-3):\n",
    "  \"\"\"Generate samples from score-based models with black-box ODE solvers.\n",
    "\n",
    "  Args:\n",
    "    score_model: A PyTorch model that represents the time-dependent score-based model.\n",
    "    marginal_prob_std: A function that returns the standard deviation\n",
    "      of the perturbation kernel.\n",
    "    diffusion_coeff: A function that returns the diffusion coefficient of the SDE.\n",
    "    batch_size: The number of samplers to generate by calling this function once.\n",
    "    atol: Tolerance of absolute errors.\n",
    "    rtol: Tolerance of relative errors.\n",
    "    device: 'cuda' for running on GPUs, and 'cpu' for running on CPUs.\n",
    "    z: The latent code that governs the final sample. If None, we start from p_1;\n",
    "      otherwise, we start from the given z.\n",
    "    eps: The smallest time step for numerical stability.\n",
    "  \"\"\"\n",
    "  t = torch.ones(batch_size, device=device)\n",
    "  # Create the latent code\n",
    "  if z is None:\n",
    "    init_x = torch.randn(batch_size, 1, 10, 10, device=device) \\\n",
    "      * marginal_prob_std(t)[:, None, None, None]\n",
    "  else:\n",
    "    init_x = z\n",
    "\n",
    "  shape = init_x.shape\n",
    "\n",
    "  def score_eval_wrapper(sample, time_steps):\n",
    "    \"\"\"A wrapper of the score-based model for use by the ODE solver.\"\"\"\n",
    "    sample = torch.tensor(sample, device=device, dtype=torch.float32).reshape(shape)\n",
    "    time_steps = torch.tensor(time_steps, device=device, dtype=torch.float32).reshape((sample.shape[0], ))\n",
    "    with torch.no_grad():\n",
    "      score = score_model(sample, time_steps)\n",
    "    return score.cpu().numpy().reshape((-1,)).astype(np.float64)\n",
    "\n",
    "  def ode_func(t, x):\n",
    "    \"\"\"The ODE function for use by the ODE solver.\"\"\"\n",
    "    time_steps = np.ones((shape[0],)) * t\n",
    "    g = diffusion_coeff(torch.tensor(t)).cpu().numpy()\n",
    "    return  -0.5 * (g**2) * score_eval_wrapper(x, time_steps)\n",
    "\n",
    "  # Run the black-box ODE solver.\n",
    "  res = integrate.solve_ivp(ode_func, (1., eps), init_x.reshape(-1).cpu().numpy(), rtol=rtol, atol=atol, method='RK45')\n",
    "  print(f\"Number of function evaluations: {res.nfev}\")\n",
    "  x = torch.tensor(res.y[:, -1], device=device).reshape(shape)\n",
    "\n",
    "  return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 445
    },
    "id": "kKoAPnr7Pf2B",
    "outputId": "bd03c292-f6b7-4e96-e5b0-001fb6b0ce26"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_48351/3030261669.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load('vmap_model.pth', map_location=device)\n",
      "/tmp/ipykernel_48351/2246265617.py:16: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(t, device=device)\n",
      "/tmp/ipykernel_48351/2246265617.py:29: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(sigma**t, device=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of function evaluations: 368\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#@title Sampling (double click to expand or collapse)\n",
    "\n",
    "\n",
    "## Load the pre-trained checkpoint from disk.\n",
    "device = 'cuda' #@param ['cuda', 'cpu'] {'type':'string'}\n",
    "ckpt = torch.load('vmap_model.pth', map_location=device)\n",
    "score_model.load_state_dict(ckpt)\n",
    "\n",
    "sample_batch_size = 64 #@param {'type':'integer'}\n",
    "sampler = ode_sampler #@param ['Euler_Maruyama_sampler', 'pc_sampler', 'ode_sampler'] {'type': 'raw'}\n",
    "\n",
    "## Generate samples using the specified sampler.\n",
    "samples = sampler(score_model,\n",
    "                  marginal_prob_std_fn,\n",
    "                  diffusion_coeff_fn,\n",
    "                  sample_batch_size,\n",
    "                  device=device)\n",
    "\n",
    "## Sample visualization.\n",
    "samples = samples.clamp(0.0, 1.0)\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.axis('off')\n",
    "plt.imshow(samples[1, 0].cpu(), vmin=0., vmax=1.)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mHsx75Yft-6u"
   },
   "source": [
    "## Further Resources\n",
    "\n",
    "If you're interested in learning more about score-based generative models, the following papers would be a good start:\n",
    "\n",
    "* Yang Song, Jascha Sohl-Dickstein, Diederik P. Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. \"[Score-Based Generative Modeling through Stochastic Differential Equations.](https://arxiv.org/pdf/2011.13456.pdf)\" International Conference on Learning Representations, 2021.\n",
    "* Jonathan Ho, Ajay Jain, and Pieter Abbeel. \"[Denoising diffusion probabilistic models.](https://arxiv.org/pdf/2006.11239.pdf)\" Advances in Neural Information Processing Systems. 2020.\n",
    "*    Yang Song, and Stefano Ermon. \"[Improved Techniques for Training Score-Based Generative Models.](https://arxiv.org/pdf/2006.09011.pdf)\" Advances in Neural Information Processing Systems. 2020.\n",
    "*   Yang Song, and Stefano Ermon. \"[Generative modeling by estimating gradients of the data distribution.](https://arxiv.org/pdf/1907.05600.pdf)\" Advances in Neural Information Processing Systems. 2019.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "1f4487629d4646459e9923bea2461cf3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.4.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.4.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.4.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4c463140b56245d98996048fd4c78b22",
      "placeholder": "​",
      "style": "IPY_MODEL_42bd42a903de4d3f8c7f741fc99695d5",
      "value": "Average bits/dim: 4.111211:   1%"
     }
    },
    "32c4f6132fca4c6797b317db5bd35938": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.1.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.1.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.1.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "42bd42a903de4d3f8c7f741fc99695d5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.4.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.4.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.1.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "488efe83bb0c4675a8f7f6c88b2796d6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.1.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.1.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.1.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4c463140b56245d98996048fd4c78b22": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.1.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.1.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.1.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "61a88f148b8e48adb16bccd7db3eeaf8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.4.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.4.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.1.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "64b0fddd0f2544468c028b6da780515c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.4.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.4.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.1.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6702c584e95347ae8d425257b8ec540e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.4.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.4.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.4.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_81a3a9c9a6b742fa9f8f19f71fedd73a",
      "placeholder": "​",
      "style": "IPY_MODEL_64b0fddd0f2544468c028b6da780515c",
      "value": " 50/50 [20:59&lt;00:00, 25.18s/it]"
     }
    },
    "6f6b56e6662149caad3e30815c25e374": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.4.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.4.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.4.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6fd034cfc2b74649ae887af76e13cbe5",
      "max": 50,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_c4de8fe7e05142a78560e13c2b1ca385",
      "value": 50
     }
    },
    "6fd034cfc2b74649ae887af76e13cbe5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.1.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.1.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.1.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7d060cf0423f48d09b5ad434e8254ec2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.4.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.4.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.1.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "81a3a9c9a6b742fa9f8f19f71fedd73a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.1.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.1.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.1.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9fe9af58fc544fce975120a09e8cc329": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.4.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.4.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.1.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "b1308d495dd643b48378ff372c8f9c3a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.4.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.4.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.4.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_c14794e9abf641a9884a0dd618e14d1e",
       "IPY_MODEL_6f6b56e6662149caad3e30815c25e374",
       "IPY_MODEL_6702c584e95347ae8d425257b8ec540e"
      ],
      "layout": "IPY_MODEL_cdf00304d2da46a1bfc2e86542882db4"
     }
    },
    "b41078ed64a24f68a69f821fc32791ad": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.1.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.1.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.1.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c14794e9abf641a9884a0dd618e14d1e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.4.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.4.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.4.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d909e838a6894af4abe248ea52bbeb93",
      "placeholder": "​",
      "style": "IPY_MODEL_61a88f148b8e48adb16bccd7db3eeaf8",
      "value": "Average Loss: 17.045697: 100%"
     }
    },
    "c4de8fe7e05142a78560e13c2b1ca385": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.4.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.4.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.1.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "cdf00304d2da46a1bfc2e86542882db4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.1.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.1.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.1.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d1b99c321d8b4256bc9c4267fbafa224": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.4.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.4.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.4.0",
      "_view_name": "ProgressView",
      "bar_style": "danger",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_32c4f6132fca4c6797b317db5bd35938",
      "max": 313,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_9fe9af58fc544fce975120a09e8cc329",
      "value": 2
     }
    },
    "d909e838a6894af4abe248ea52bbeb93": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.1.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.1.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.1.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "db075f0871564f5da3119833dcffbf17": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.4.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.4.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.4.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_1f4487629d4646459e9923bea2461cf3",
       "IPY_MODEL_d1b99c321d8b4256bc9c4267fbafa224",
       "IPY_MODEL_dded254d850e4f86a778aa73439ebc48"
      ],
      "layout": "IPY_MODEL_488efe83bb0c4675a8f7f6c88b2796d6"
     }
    },
    "dded254d850e4f86a778aa73439ebc48": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.4.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.4.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.4.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b41078ed64a24f68a69f821fc32791ad",
      "placeholder": "​",
      "style": "IPY_MODEL_7d060cf0423f48d09b5ad434e8254ec2",
      "value": " 2/313 [00:19&lt;50:11,  9.68s/it]"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
